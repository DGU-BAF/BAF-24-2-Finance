{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "(author) Jang HS\n",
    "(version) 1.0, 초기 버전\n",
    "(date) 20200830\n",
    "'''\n",
    "\n",
    "# ==========================================================\n",
    "# 0. Package Load\n",
    "# ==========================================================\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import sys  \n",
    "import time\n",
    "import random\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# ==========================================================\n",
    "# 1. 필수 입력\n",
    "# ==========================================================\n",
    "\n",
    "# [지역 선택] 전국 / 서울, 경기, 인천, 부산, 대구, 광주, 대전, 울산, 세종, 강원, 경남, 경북, 전남, 전북, 충남, 충북, 제주, 전국\n",
    "# 입력 예시 ['서울', '경기'] ['전국'] ['대구', '부산']\n",
    "area_select = ['서울', '경기', '인천', '부산', '대구', '광주', '대전', '울산', '세종', '강원', '경남', '경북', '전남', '전북', '충남', '충북', '제주', '전국']\n",
    "\n",
    "# # [검색어] 코드 실행시 입력\n",
    "# search_word = input(\"검색어를 입력하세요 : \")\n",
    "# search_word = str(search_word)\n",
    "\n",
    "# [채용 정보] 일반 채용정보 / 파견.대행 / 헤드헌팅 중 선택하는 경우를 리스트로 받음 (최소 1개 이상 선택)\n",
    "# 일반 채용정보, 파견.대행 선택시 ['Y', 'Y', 'N']\n",
    "cha = ['Y', 'N', 'N']\n",
    "\n",
    "# # [정렬 순서] 1 관련도 / 2 정확도/ 3 등록일 / 4 수정일 / 5 마감일 / 6 지원자 / 7 사원수\n",
    "# orderby = 2\n",
    "\n",
    "# ==========================================================\n",
    "# 2. webdriver 실행, 검색어 입력\n",
    "# ==========================================================\n",
    "driver = webdriver.Chrome(executable_path=r'/Users/chromedriver')\n",
    "driver.get(\"https://www.saramin.co.kr/zf_user/\")\n",
    "\n",
    "#검색창 clear\n",
    "elem = driver.find_element_by_id('ipt_keyword_recruit')\n",
    "elem.clear()\n",
    "\n",
    "#검색어 입력\n",
    "elem.send_keys(search_word)\n",
    "\n",
    "# 검색클릭\n",
    "elem = driver.find_element_by_id(\"btn_search_recruit\")\n",
    "elem.click()\n",
    "\n",
    "time.sleep(random.randint(0, 1))\n",
    "\n",
    "# ==== 정렬 순서\n",
    "# 정렬 순서 열기\n",
    "driver.implicitly_wait(5)\n",
    "elem = driver.find_element_by_xpath('//*[@id=\"recruit_info\"]/div[2]/div/div[2]/button')\n",
    "elem.click()\n",
    "\n",
    "# 정렬 순서\n",
    "order_xpath = '//*[@id=\"recruit_info\"]/div[2]/div/div[2]/div/ul/li[{}]/button'.format(str(orderby))\n",
    "\n",
    "# 정렬 순서 클릭\n",
    "elem = driver.find_element_by_xpath(order_xpath)\n",
    "driver.execute_script(\"arguments[0].click();\", elem)\n",
    "# elem.click()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3. 게시물 100개 선택 / 지역 선택\n",
    "# ==========================================================\n",
    "\n",
    "# 채용정보 더보기 후 첫페이지로 이동\n",
    "elem = driver.find_element_by_xpath('//*[@id=\"recruit_info_list\"]/div[2]/div/a')\n",
    "driver.execute_script(\"arguments[0].click();\", elem)\n",
    "# elem.click()\n",
    "\n",
    "elem = driver.find_element_by_xpath('//*[@id=\"recruit_info_list\"]/div[2]/div/a[1]')\n",
    "driver.execute_script(\"arguments[0].click();\", elem)\n",
    "# elem.click()\n",
    "driver.execute_script(\"window.scrollTo(0, 0);\") # 오류 방지를 위한 페이지 상단으로 이동\n",
    "\n",
    "\n",
    "# 처음 100개를 받아오기\n",
    "time.sleep(random.randint(0, 1))\n",
    "elem = driver.find_element_by_xpath(\"//*[@id='recruit_info']/div[2]/div/div[3]/button\")\n",
    "driver.execute_script(\"arguments[0].click();\", elem)\n",
    "# elem.click()\n",
    "\n",
    "elem = driver.find_element_by_xpath(\"//*[contains(text(), '100개씩')]\")\n",
    "driver.execute_script(\"arguments[0].click();\", elem)\n",
    "# elem.click()\n",
    "\n",
    "driver.implicitly_wait(3)\n",
    "area_path_table = pd.read_csv('area_path_table.csv')\n",
    "area_path_table['selected']  = area_path_table['area'].map(lambda x : 1 if '전국' in area_select else 1 if x in area_select else 0)\n",
    "area_path_table\n",
    "\n",
    "# 지역 선택 \n",
    "driver.execute_script(\"window.scrollTo(0, 0);\") # 오류 방지를 위한 페이지 상단으로 이동\n",
    "elem = driver.find_element_by_xpath('//*[@id=\"sp_main_wrapper\"]/div[2]/ul/li[2]/button')\n",
    "elem.click()\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "\n",
    "\n",
    "# 선택된 지역 기준으로 채용 공고를 선택하게 만들기.\n",
    "area_list = list(area_path_table[area_path_table['selected']==1]['xpath'])\n",
    "\n",
    "\n",
    "if '전국' in area_select :\n",
    "    print(\"선택 지역: 전국\")\n",
    "\n",
    "else :\n",
    "    print(\"선택 지역: {}\".format(str(area_select)))\n",
    "    for area in area_list:\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "        elem = driver.find_element_by_xpath(area+'/button')\n",
    "        driver.execute_script(\"arguments[0].click();\", elem)\n",
    "        # elem.click()\n",
    "\n",
    "# ==========================================================\n",
    "# 4. 조건에 따른 채용 정보 선택\n",
    "# ==========================================================\n",
    " \n",
    "# 휴식 및 페이지 상단으로 이동    \n",
    "driver.implicitly_wait(3)\n",
    "driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "\n",
    "# 일반 채용정보 / 파견.대행 / 헤드헌팅 중 선택\n",
    "# 전체 채용 정보일 경우 pass로\n",
    "if cha == ['Y', 'Y', 'Y'] :\n",
    "    pass\n",
    "else :\n",
    "    # 최소한 하나 이상의 채용 조건을 선택해야 함\n",
    "    if cha[0] + cha[1] + cha[2] == 'NNN' : \n",
    "        print('error')\n",
    "\n",
    "    # 일반 채용정보 선택 해제 \n",
    "    elem = driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    if cha[0] != 'Y' :\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"recruit_info\"]/div[1]/div/div[1]')\n",
    "        driver.execute_script(\"arguments[0].click();\", elem)\n",
    "\n",
    "    elem = driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    if cha[1] != 'Y' :\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"recruit_info\"]/div[1]/div/div[2]')\n",
    "        driver.execute_script(\"arguments[0].click();\", elem)\n",
    "\n",
    "    elem = driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    if cha[2] != 'Y' :\n",
    "        elem = driver.find_element_by_xpath('//*[@id=\"recruit_info\"]/div[1]/div/div[3]')\n",
    "        driver.execute_script(\"arguments[0].click();\", elem)\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5. 채용 공고 크롤링\n",
    "# ==========================================================\n",
    " \n",
    "\n",
    "\n",
    "full_dataset = pd.DataFrame(columns = ['title', 'title_link', 'end_date', 'text_info','work_keyword','office_name'])\n",
    "\n",
    "time.sleep(7)\n",
    "\n",
    "# 채용공고 수집 시작\n",
    "for item_num in tqdm(range(1,101)):\n",
    "    try : \n",
    "        # 공고 제목 데이터 선택\n",
    "        path_ = '//*[@id=\"recruit_info_list\"]/div[1]/div[{}]'.format(str(item_num))\n",
    "        \n",
    "        # 공고명\n",
    "        title = driver.find_element_by_xpath(path_ +'/div[1]/h2/a').get_attribute(\"title\")\n",
    "\n",
    "        # 공고링크\n",
    "        title_link = driver.find_element_by_xpath(path_ +'/div[1]/h2/a').get_attribute(\"href\")\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "        # 공고 마감일\n",
    "        end_date = driver.find_element_by_xpath(path_+'/div[1]/div[2]/span').text\n",
    "        \n",
    "        # 공고 지역\\n경력\\n학력\\n직업 종류 + 연봉 있는경우\n",
    "        text_info = driver.find_element_by_xpath(path_+'/div[1]/div[3]').text\n",
    "\n",
    "        # 업종 ' 전까지는 업종 키워드\n",
    "        work_keyword = driver.find_element_by_xpath(path_+'/div[1]/div[4]')\n",
    "        work_keyword = work_keyword.text\n",
    "\n",
    "        # 기업명 text\n",
    "        office_name = driver.find_element_by_xpath(path_+'/div[2]/strong/a').get_attribute(\"title\")\n",
    "        if len(office_name) == 0 :\n",
    "            office_name = driver.find_element_by_xpath(path_+'/div[2]/strong/a/span').text\n",
    "\n",
    "        temp_dataset = pd.DataFrame({'title':[title], 'title_link':[title_link], 'end_date':[end_date], 'text_info':[text_info],'work_keyword':[work_keyword],'office_name':[office_name]})\n",
    "        \n",
    "        full_dataset = pd.concat([full_dataset, temp_dataset], axis = 0)\n",
    "    \n",
    "    except : print(\"no content in item no {}\".str(item_num)) \n",
    "\n",
    "time.sleep(random.randint(0, 1))\n",
    "full_dataset = full_dataset.reset_index(drop=True)\n",
    "full_dataset.head()\n",
    "\n",
    "# webdriver 닫기\n",
    "driver.quit()\n",
    "\n",
    "# ==========================================================\n",
    "# 6. 수집 데이터 전처리 및 저장\n",
    "# ==========================================================\n",
    " \n",
    "# 날짜 포맷을 맞추기\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "end_date_after = []\n",
    "for row_num in range(len(full_dataset)):\n",
    "    end_date_before = full_dataset['end_date'][row_num]\n",
    "    \n",
    "    # 오늘\n",
    "    if '오늘' in end_date_before:\n",
    "        end_date_after.append(datetime.today().strftime(\"%Y/%m/%d\"))\n",
    "    elif '내일' in end_date_before:\n",
    "        end_date_after.append((datetime.today() + timedelta(1)).strftime(\"%Y/%m/%d\"))\n",
    "    elif '상시' in end_date_before:\n",
    "        end_date_after.append('상시')\n",
    "    elif '채용시'  in end_date_before:\n",
    "        end_date_after.append('채용시')\n",
    "\n",
    "    elif '~' in end_date_before:\n",
    "        year = datetime.today().year\n",
    "        month = full_dataset['end_date'][row_num][2:4]\n",
    "        day = full_dataset['end_date'][row_num][5:7]\n",
    "        date_after = str(year)+str(month)+str(day)\n",
    "        end_date_after.append(pd.to_datetime(date_after).strftime(\"%Y/%m/%d\"))\n",
    "    \n",
    "    else : end_date_after.append(str(end_date_before))\n",
    "        \n",
    "\n",
    "full_dataset['end_date'] = end_date_after\n",
    "full_dataset.head()\n",
    "\n",
    "# 지역 / 경력 / 학력 / 검색 키워드 column 생성\n",
    "area =[]\n",
    "level = []\n",
    "school = []\n",
    "keyword = []\n",
    "for row_num in range(len(full_dataset)):\n",
    "    area_ = full_dataset['text_info'][row_num].split(sep='\\n')[0]\n",
    "    level_ = full_dataset['text_info'][row_num].split(sep='\\n')[1]\n",
    "    school_ =full_dataset['text_info'][row_num].split(sep='\\n')[2]\n",
    "    keyword_ = full_dataset['work_keyword'][row_num].split(sep=\"'\")[0].strip()\n",
    "    \n",
    "    area.append(area_)\n",
    "    level.append(level_)\n",
    "    school.append(school_)\n",
    "    keyword.append(keyword_)\n",
    "\n",
    "\n",
    "full_dataset['area']= area\n",
    "full_dataset['level']= level\n",
    "full_dataset['school']= school\n",
    "full_dataset['keyword']= keyword\n",
    "\n",
    "full_dataset = full_dataset.drop(['text_info','work_keyword'],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# csv형태로 저장\n",
    "full_dataset = full_dataset[['title','title_link','area','level','school','end_date','office_name','keyword']]\n",
    "full_dataset.to_csv('saramin_crawler.csv',encoding='euckr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-1.1.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-1.1.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "원티드 \n",
    "BASE_URL = \"https://www.wanted.co.kr/wd/\" (1001 ~ 47682) \n",
    "\"\"\"\n",
    " \n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl import Workbook\n",
    " \n",
    " \n",
    "POSTING_NUM_LIST = []\n",
    "JOB_DESC_LIST = []                 # 공고내용 (col-md-12)\n",
    "TITLE_LIST = []                 # 채용공고 제목 (tm_mgt_title)\n",
    "COMPANY_NAME_LIST = []             # 회사이름 (tm_h2_title_company_info)\n",
    "CATEGORY_LIST = []                 # 부문 (rc_categories_name)\n",
    "URL_LIST = []\n",
    " \n",
    " \n",
    "# (1001 ~ 47664) 천에서 4.7만 (거의 5만개)\n",
    "# 시작:1001\n",
    "# 끝: 47682\n",
    " \n",
    "def MAKE_URL():\n",
    "  for i in range(1001, 1101, 1):\n",
    "    URL = \"https://www.wanted.co.kr/wd/\" + str(i)\n",
    "    URL_LIST.append(URL)\n",
    " \n",
    " \n",
    "# M A I N \n",
    "MAKE_URL()\n",
    " \n",
    "ABC = [\"A1\", \"B1\", \"C1\", \"D1\"]\n",
    "columns = [\"회사이름\", \"직무\", \"유사직무\", \"채용내용\"]\n",
    " \n",
    "write_wb = Workbook()\n",
    "write_ws = write_wb.active\n",
    " \n",
    "# Head Columns 만들기\n",
    "for (alphabet, col) in zip(ABC, columns): \n",
    "  write_ws[alphabet] = col\n",
    " \n",
    " \n",
    "for i, URL in enumerate(URL_LIST):\n",
    "  response = requests.get(URL)\n",
    "  html = response.text\n",
    "  soup = BeautifulSoup(html, 'lxml')\n",
    "  soup = str(soup)\n",
    " \n",
    "  jikmoo = soup[soup.find('\"position\":\"') + 12 : soup.find('\"reward\":') - 2]\n",
    "  # print(\"직무:\", jikmoo)\n",
    "  yusa_jikmoo = soup[soup.find('\"sub_categories\":') + 18 : soup.find('\"position\":\"') - 2]\n",
    "  # print(\"유사직무:\", yusa_jikmoo)\n",
    "  job_naeyong = soup[soup.find('\"jd\":') + 5 : soup.find('\"company_name\":\"') - 2]\n",
    "  # print(\"채용내용:\", job_naeyong)\n",
    "  company_name = soup[soup.find('\"company_name\":\"') + 16 : soup.find('\"lang\":\"') - 2]\n",
    "  # print(\"회사이름:\", company_name)\n",
    "  \n",
    "  write_ws.append([ \n",
    "                    company_name, \n",
    "                    jikmoo, \n",
    "                    yusa_jikmoo, \n",
    "                    job_naeyong \n",
    "                 ])\n",
    " \n",
    " \n",
    "write_wb.save(\"Wanted.csv\") # save csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
